{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19091793-7e67-4136-8a02-ba920e088803",
   "metadata": {},
   "source": [
    "<h1>Overview</h1>\n",
    "<hr>\n",
    "\n",
    "We will be going over three areas of the RAG pipeline in this lab. 1. Data Prep 2. Embedding 3. VectorDB and finish with retrieving to show a complete pipeline implementation. Keep in mind this is designed as an intermidiate aproach to the RAG pipeline by pulling back the cover and seeing some of the processes, code and technologies used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ffa4aa-325a-4562-a944-418ab4538be0",
   "metadata": {},
   "source": [
    "<h1>Data Prep (chunking/splitting)</h1>\n",
    "<hr>\n",
    "\n",
    "Data chunking in the context of Retrieval Augmented Generation (RAG) refers to the process of breaking down large documents or datasets into smaller, more manageable units of information.  These units, called \"chunks,\" are designed to be relevant and self-contained enough to be useful for retrieval and subsequent processing by a large language model (LLM).  The goal is to find the right sized chunks â€“ small enough to be specific and relevant, but large enough to contain sufficient context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38b39ac-dcbe-4994-8146-d3b8ea0fe6b1",
   "metadata": {},
   "source": [
    "In the following example, we'll demonstrate a simple method of how to split a file of text into individual chunks using the [langchain.text_splitter](https://python.langchain.com/api_reference/text_splitters/index.html) library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57225c63-37b4-40e4-8ecb-95dd5fbe2e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c8700f-b3f4-4bb7-b8d3-b36817a9dc4c",
   "metadata": {},
   "source": [
    "Let's take a look at the library CharacterTextSplitter and see what it does with the text. We will be setting the chunk_size to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78398d62-fb2c-47a2-ba26-0eddc970f3ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "documents = []  # The data is returned into document objects which is of type list.\n",
    "\n",
    "loader = TextLoader(\"data/dissimilar.txt\") # This is a text file located in the data directory\n",
    "docs = loader.load()\n",
    "documents += docs\n",
    "\n",
    "documents[0].page_content # This is displaying that the contents have been loaded into the document object."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f96a784-2d95-4984-82b4-c00a4fd65254",
   "metadata": {},
   "source": [
    "<i>The TextLoader module from langchain returns the content as a Document object.  This object is comprised of the actual data and metadata that can be used for further classification.  An example would be the source of the data like the filename or web site the data was retrieved from. </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6912b9-9a3d-43c3-b603-c66ea952ad54",
   "metadata": {},
   "source": [
    "As mentioned above all the data from the file was loaded into a single document object, this is becuase we did not perform any chunking or splitting of the text we just loaded it.  Here we will verfiy there is a single document created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73506bc6-26df-409c-80b4-5666a8fd404d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0c9cde-6b08-44de-bff3-6776a2f7c452",
   "metadata": {},
   "source": [
    "Below will show the metadata that it had added to the document when the data was loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d323738f-4150-4b4b-9c9d-47757c1247e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8691cda5-c2a9-4a03-8e28-6bbb71e61808",
   "metadata": {},
   "source": [
    "Below will show the content of the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c9898b-c955-4b7f-992c-8edec4bc6d28",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "documents[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd26194d-2947-426c-ab86-f8172f6e6a09",
   "metadata": {},
   "source": [
    "As you can see the document needs to be cleaned up a little to remove any unnecessary characters, newlines, etc. This is part of the process of preparing the data for retrievel and provide more context aware results. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c4ac17-2df7-4059-97a8-d809ee2a8d81",
   "metadata": {},
   "source": [
    "<h2>CharacterTextSplitter</h2>\n",
    "<hr>\n",
    "\n",
    "langchain provides modules for this under the textsplitters.character class. The first one we will look at is the CharacterTextSplitter.\n",
    "\n",
    "Let us setup the [CharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.CharacterTextSplitter.html#langchain_text_splitters.character.CharacterTextSplitter) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d8cb9f-dbc9-419f-ad3b-4f8fc43c3bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91687d24-0c3a-498d-97af-5c357ce7b889",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=10,   # Setting the number of characters to split on\n",
    "    chunk_overlap=0, # Using 0 as the overlap\n",
    "    separator=\"\"     # setting this to split on each character\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6569b3-328d-4c34-95d1-7c9ea80cf29a",
   "metadata": {},
   "source": [
    "Now that the data has been split into chunks there are a lot more documents created.  As you can see the content of each document is not a semantic chunk as it has no meaning or context.  This is an example of hard splitting on a set number of charcters, as you can see this doesn't provide much help. \n",
    "\n",
    "Lets take a look at the content in the top 10 documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea05774-c321-46b1-944c-66dfc761df1d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcab099-1599-4a91-8f6e-b7bec28426c0",
   "metadata": {},
   "source": [
    "The content in each document is split by characters based on the amount defined in chunk_size.  It counts all characters (newlines, returns, blanks, characters, etc...) which you can see in the results.  \n",
    "\n",
    "As you can see the output does not provide useful information, as it splits in the middle of a word, there is no meaning or context in these documents.  Our next step is to put some meaning to the data.  Instead of hard character splitting we will split on sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc72b28-0881-49fb-95de-5c3007335e68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(\n",
    "    chunk_size=10,\n",
    "    chunk_overlap=0,\n",
    "    separator='.'  # This will separate on sentences\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de4dfd8-a7ba-4728-9d5b-418f058ca960",
   "metadata": {},
   "source": [
    "You will receive messages that indicate that a chunk size was created that is larger than defined. Those are due to how the separator and chunk size work together. For now those can be ignored.\n",
    "\n",
    "Preserving context at the chunk boundaries invloves using `chunk_overlap` which allows you to specify how much overlap there should be between consecutive chunks. This means that some of the text from the end of one chunk will be repeated at the beginning of the next chunk. Overlapping chunks help maintain the flow of information and ensure that sentences or concepts that are split are still understood in their entirety.\n",
    "\n",
    "Since the data being used in the sample has no semantic overlap between sentences the `chunk_overlap` argument is set to 0.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b1eb48-891a-4cf4-ab3c-e8166e433595",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>Click here for an explination of ChracterTextSplitter</summary>\n",
    "Let's break down how they work together:\n",
    "\n",
    "**`chunk_size`**\n",
    "\n",
    "*   **Maximum Chunk Length:** This parameter sets the *maximum* number of characters that a single chunk of text can contain. Think of it as a target length for your chunks.\n",
    "*   **Not a Strict Limit:** It's important to understand that `chunk_size` is not a hard limit. The splitter will try to create chunks around this size, but it might produce chunks that are slightly longer if it can't find a suitable split point within the `chunk_size` limit.\n",
    "\n",
    "**`chunk_overlap`**\n",
    "\n",
    "* This determines how much overlap there should be between consecutive chunks. This helps preserve context at the boundaries. (Default: 0)\n",
    "\n",
    "\n",
    "**`separator`**\n",
    "\n",
    "*   **Where to Split:** This parameter specifies the character or string that the splitter should use to divide the text into chunks. Common separators include:\n",
    "    *   `\\n\\n` (double newline, often used to separate paragraphs)\n",
    "    *   `\\n` (single newline, often used to separate sentences)\n",
    "    *   `.` (period, to split at sentence boundaries)\n",
    "    *   ` ` (space, to split at word boundaries)\n",
    "*   **Priority:** The splitter will prioritize splitting the text at the specified separator. It will try to make chunks that end with the separator, as long as they are within the `chunk_size`.\n",
    "\n",
    "**How They Interact**\n",
    "\n",
    "1.  **Splitting at Separators:** The `CharacterTextSplitter` first looks for the `separator` within the text.\n",
    "2.  **Chunk Size Check:** It then checks if the text between those separators is less than or equal to the `chunk_size`.\n",
    "3.  **Creating Chunks:**\n",
    "    *   If the text is within the limit, it creates a chunk.\n",
    "    *   If the text exceeds the limit, it will try to find another `separator` within the `chunk_size` range to split the text. If no suitable `separator` is found, the chunk will be larger than `chunk_size`.\n",
    "\n",
    "**Key Points**\n",
    "\n",
    "*   The `CharacterTextSplitter` aims to create chunks that are close to `chunk_size` but will respect the `separator` where possible.\n",
    "*   If no suitable `separator` is found within the `chunk_size`, chunks might exceed the specified size.\n",
    "*   Choosing the right `separator` and `chunk_size` depends on the nature of your text and how you plan to use the chunks.\n",
    "*   `chunk_overlap` allows you to specify how much overlap there should be between consecutive chunks. This means that some of the text from the end of one chunk will be repeated at the beginning of the next chunk.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0172399-78df-49d1-9870-8fe7a12e46e2",
   "metadata": {},
   "source": [
    "Lets take a look at the number of documents created as well as a snippet of the document contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3bc31aa-2f95-40c3-84b9-2628390f727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228fbf0c-8acc-433f-bd39-9efad6f4b827",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26cc4f65-38f1-49e3-9502-dedde9e03a42",
   "metadata": {},
   "source": [
    "Since we used the '.' as a separaotr you can see the output makes a little more sense than before as it is chunking at the end of a sentence rather than the middle of a word. But you will need to pay attention to the data, as an example \"Dr.\" would split on the \".\" and then leave some non-semantiac data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28228967-0b22-4a04-9a3b-87431e39d448",
   "metadata": {},
   "source": [
    "<h2>RecursiveCharacterTextSplitter</h2>\n",
    "<hr>\n",
    "\n",
    "Because there is more than one way to split the data langchain has a function called [RecursiveCharacterTextSplitter](https://python.langchain.com/api_reference/text_splitters/character/langchain_text_splitters.character.RecursiveCharacterTextSplitter.html#recursivecharactertextsplitter) which provides a method to use mulitple separators in a recursive manner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f91b926-54bf-4350-a5d6-11b734cab114",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dcdc72-efed-416a-b1e8-ec4e4d159cb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "docs[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f8a240-2828-4929-85c4-128baf00c285",
   "metadata": {},
   "source": [
    "Using this function fixes the \"Dr.\" problem from earlier and splits the data in a more context aware fashion. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d23e88-5944-4a4b-ae69-a511de17b5d5",
   "metadata": {},
   "source": [
    "<details>\n",
    " <summary>Here's a breakdown of the key differences between the two functions</summary>\n",
    "\n",
    "**CharacterTextSplitter**\n",
    "\n",
    "*   **Simple Splitting:** This is the more basic approach. It splits the text based on a separator you define (e.g., `\\n\\n` for paragraphs, `\\n` for sentences, or even a specific character).\n",
    "*   **Fixed Chunking:** It tries to create chunks around a `chunk_size` you specify, but it might not always be exact. If it can't find your separator within the `chunk_size`, it might create a chunk that's a bit longer.\n",
    "*   **Less Context Awareness:** It primarily focuses on the separator and `chunk_size`, without necessarily considering the semantic meaning or relationships between different parts of the text.\n",
    "\n",
    "**RecursiveCharacterTextSplitter**\n",
    "\n",
    "*   **Hierarchical Splitting:** This method is more sophisticated. It splits the text recursively, trying to keep related pieces of information together. It starts by splitting on the most significant separators (like `\\n\\n` for paragraphs), then moves down to less significant ones (like `\\n` for sentences), and so on.\n",
    "*   **Context Preservation:** It aims to maintain context by prioritizing splits at logical boundaries. This is especially useful for longer documents where you want to ensure that related sentences or paragraphs stay within the same chunk.\n",
    "*   **More Flexible:** It offers more control over how the text is split, allowing you to define a list of separators to try in order.\n",
    "\n",
    "**Here's a table summarizing the key differences:**\n",
    "\n",
    "| Feature | CharacterTextSplitter | RecursiveCharacterTextSplitter |\n",
    "|---|---|---|\n",
    "| Splitting Approach | Simple, based on separator | Hierarchical, recursive |\n",
    "| Context Awareness | Lower | Higher |\n",
    "| Chunking | Tries to match `chunk_size` | Prioritizes logical boundaries |\n",
    "| Flexibility | Less | More |\n",
    "\n",
    "**When to Use Which**\n",
    "\n",
    "*   **`CharacterTextSplitter`:** Suitable for simpler tasks where you just need to break the text into manageable chunks, and context preservation is not critical.\n",
    "*   **`RecursiveCharacterTextSplitter`:** Recommended for more complex scenarios, especially when dealing with longer documents or when it's important to maintain the relationships between different parts of the text. This is often the better choice for tasks like question answering or summarization, where understanding the context is crucial.\n",
    "\n",
    "**In essence:** The `RecursiveCharacterTextSplitter` is like a more intelligent version of the `CharacterTextSplitter`. It's designed to create chunks that are not only of a reasonable size but also make sense semantically.\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8859e76c-1bed-426b-b19c-37dd30d12dc0",
   "metadata": {},
   "source": [
    "Here is a link to a site [graphical site](https://chunkviz.up.railway.app/) and a screenshot that will visually show you chunk size and chunk overlap on text.  This is useful to get an understanding of chunk_size and chunk_overlap on your data. \n",
    "\n",
    "![screeshot](images/chunkviz-screenshot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21f68cc-5509-4bc2-8fbb-00491413ce74",
   "metadata": {},
   "source": [
    "<h1>Embedding</h1>\n",
    "<hr>\n",
    "\n",
    "The data is in a good place to continue on to the next phase which is embedding. Embedding data refers to the process of converting data into a numerical representation called an \"embedding vector.\"  This vector captures the semantic meaning or characteristics of the data in a way that can be understood and used by machine learning models, especially large language models (LLMs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbe0204-5070-4432-93b1-1c48d5ef6329",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "\n",
    "<summary>More details on Embedding Vectors.</summary>\n",
    "\n",
    "Imagine you have words like \"king,\" \"queen,\" \"man,\" and \"woman.\"  A word embedding model would represent each of these words as a vector of numbers (e.g., `[0.25, 0.78, -0.12, ...]`).  The key is that these vectors are designed so that:\n",
    "\n",
    "*   **Semantically Similar Items are Close:** The vectors for \"king\" and \"man\" would be closer to each other in \"vector space\" than the vectors for \"king\" and \"woman.\"  \"Queen\" and \"woman\" would also be close.  This reflects the semantic relationships between the words.\n",
    "*   **Relationships are Encoded:** The *differences* between the vectors can also be meaningful. For example, the difference between the \"king\" and \"man\" vectors might be similar to the difference between the \"queen\" and \"woman\" vectors, capturing the concept of \"gender.\"\n",
    "\n",
    "**Why Embed Data?**\n",
    "\n",
    "*   **Machine Learning Understanding:** Machine learning models, particularly neural networks, work with numbers.  Embeddings provide a way to translate complex data like text, images, or even audio into a numerical form that these models can understand and process.\n",
    "*   **Semantic Representation:** Embeddings capture the underlying meaning and relationships between data points. This is crucial for tasks like:\n",
    "    *   **Similarity Search:** Finding items that are related to each other (e.g., finding similar articles, products, or images).\n",
    "    *   **Clustering:** Grouping similar items together.\n",
    "    *   **Recommendation Systems:** Recommending items that a user might like based on their past behavior.\n",
    "    *   **Natural Language Processing (NLP):** Understanding the meaning and context of text.\n",
    "*   **Dimensionality Reduction:** Embeddings can often represent complex data in a lower-dimensional space, making it easier to work with and reducing computational costs.\n",
    "\n",
    "**How Does it Work?**\n",
    "\n",
    "The process of creating embeddings typically involves training a machine learning model on a large dataset.  For text, models like Word2Vec, GloVe, and BERT are commonly used.  These models learn to associate words or phrases with vectors in such a way that the relationships described above are captured.\n",
    "\n",
    "**Example (Conceptual):**\n",
    "\n",
    "Let's imagine a simplified 2D embedding space (in reality, embedding vectors have many more dimensions).\n",
    "\n",
    "```\n",
    "      Woman\n",
    "         |\n",
    "         |\n",
    "         |\n",
    "Queen----|----King\n",
    "         |\n",
    "         |\n",
    "      Man\n",
    "```\n",
    "\n",
    "In this example, the position of each word represents its embedding vector.  You can see how \"king\" and \"man\" are close, as are \"queen\" and \"woman.\"\n",
    "\n",
    "**In Summary**\n",
    "\n",
    "Embedding data is the process of converting data into numerical vectors that capture its semantic meaning.  These embeddings are essential for enabling machine learning models to understand and work with complex data, leading to a wide range of applications in NLP, information retrieval, and other fields.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1248049f-4bdb-4732-9481-aadfecdc2a68",
   "metadata": {},
   "source": [
    "Here we are setting up what embedding model to use to store the documents with.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91900f63-a4aa-45ab-9c86-1ce581c02f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings(\n",
    "    model=\"llama3.1:latest\",\n",
    "    base_url=\"192.168.15.91:11434\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9472a7-c2ed-41d6-8ba6-b87e7c97d835",
   "metadata": {},
   "source": [
    "And for fun lets do a test embedding, first we will see the vectors created for a single word. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e01268c-9ceb-4198-a42c-b8e45b8be3a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentence = \"Hello\"\n",
    "\n",
    "embedding_vector = embeddings.embed_query(sentence)\n",
    "\n",
    "print(embedding_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362d30a0-8f64-498f-a301-792d09292172",
   "metadata": {},
   "source": [
    "Let's see how many vectors have been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc7fa80-f36c-460f-920e-cdc9b83320c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embedding_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2005a82-6aab-408e-a069-0718bafdff50",
   "metadata": {},
   "source": [
    "Let's see how many vectors are created for a full sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f75e57-b138-4f8e-a063-96ed0aa59ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"This is a test sentence to show the number of vectors created.\"\n",
    "\n",
    "embedding_vector = embeddings.embed_query(sentence)\n",
    "\n",
    "len(embedding_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583ca4a1-769f-4c04-80d6-28a927623d75",
   "metadata": {},
   "source": [
    "As you can see the same number of vectors have been created for either a single word or an entire sentence.  The number of vectors created is based on the model used to do the embeddding.  <i>On a side note, you will need to use the same model to retrieve as you did to embed you documents.</i> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaee23c3-ac46-4800-abdb-90e1e0ea2aa4",
   "metadata": {},
   "source": [
    "<h1>Vector Database</h1>\n",
    "<hr>\n",
    "\n",
    "Let us continue with embedding and storing the results.  To store the vectors created by embedding we will use a vector database.  These type of databases differ from traditional in the sense that they store vectors in high-dimensional space and perform similarity searching versus exact matches against structured data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62438f67-2a9d-4c09-a1ea-e5da610f0096",
   "metadata": {},
   "source": [
    "<details>\n",
    "   <summary>Here's a breakdown of the key differences between a traditional database and a vector database</summary>\n",
    "\n",
    "**Traditional Databases**\n",
    "\n",
    "*   **Structure:** Organize data into tables with rows (records) and columns (attributes). Think of a spreadsheet.\n",
    "*   **Data Types:** Best suited for structured data like numbers, dates, and short strings.\n",
    "*   **Queries:** Designed for exact matches, range queries, and aggregations (e.g., \"Find all customers with age between 25 and 30\").\n",
    "*   **Indexing:** Use indexes to speed up searches based on specific columns (e.g., an index on customer ID).\n",
    "*   **Examples:** MySQL, PostgreSQL, Oracle\n",
    "\n",
    "**Vector Databases**\n",
    "\n",
    "*   **Structure:** Store data as vectors (arrays of numbers) in a high-dimensional space. These vectors represent the meaning or features of the data.\n",
    "*   **Data Types:** Optimized for unstructured data like text, images, audio, and video, which are converted into vectors using embedding models.\n",
    "*   **Queries:** Designed for similarity searches (e.g., \"Find the most similar images to this one\").\n",
    "*   **Indexing:** Use specialized indexing techniques (like Approximate Nearest Neighbor search) to efficiently find similar vectors in high-dimensional space.\n",
    "*   **Examples:** LanceDB, Chroma, Pinecone, Weaviate, Milvus\n",
    "\n",
    "**Here's a table summarizing the key differences:**\n",
    "\n",
    "| Feature | Traditional Database | Vector Database |\n",
    "|---|---|---|\n",
    "| **Data Organization** | Tables with rows and columns | Vectors in high-dimensional space |\n",
    "| **Data Types** | Structured (numbers, dates, strings) | Unstructured (text, images, audio) |\n",
    "| **Query Type** | Exact matches, range queries | Similarity searches |\n",
    "| **Indexing** | Standard indexes on columns | Specialized indexes for vector similarity |\n",
    "\n",
    "**Why the Difference Matters**\n",
    "\n",
    "*   **Different Use Cases:** Traditional databases are great for managing structured data and performing precise queries. Vector databases excel at handling unstructured data and finding items that are semantically similar.\n",
    "*   **Performance:** Vector databases are optimized for the kind of queries that are essential for AI applications (like recommendation systems, semantic search, and RAG), which traditional databases would struggle with.\n",
    "\n",
    "**In Simple Terms**\n",
    "\n",
    "Imagine you have a library:\n",
    "\n",
    "*   **Traditional Database:** It's like the card catalog, where you can look up books by title, author, or subject.\n",
    "*   **Vector Database:** It's like having a map where books on similar topics are located close together. You can point to a topic on the map and instantly find all the related books.\n",
    "\n",
    "**Key Takeaway:** Vector databases are a new kind of database designed specifically to handle the unique challenges of working with embeddings and performing similarity searches, which are fundamental to many AI applications.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af28575-8e5b-4c3c-a414-9574816b04be",
   "metadata": {},
   "source": [
    "Here we will use LanceDB as our vector database for this example.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdd8b70-ea86-459e-8805-b002f12657eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import LanceDB\n",
    "\n",
    "import lancedb\n",
    "\n",
    "db = lancedb.connect(\"data/lancedb\") # Connect and create a persistent store for the database\n",
    "\n",
    "vectordb = LanceDB.from_documents(\n",
    "    docs, \n",
    "    embeddings, \n",
    "    connection=db,\n",
    "    table_name=\"lab_embeddings\",\n",
    "    mode=\"overwrite\"\n",
    ")  # This wrapper will embed and store the results in the table of the vector database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c7b88e-77c7-4693-9697-ce6b8a12daec",
   "metadata": {},
   "source": [
    "The documents have now been embeded and the results have been stored in the vector database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17cb0286-8e3a-4746-98c8-4dd6afd606ba",
   "metadata": {},
   "source": [
    "Now we will do a simple retrieval to verify it can get to the data that is stored in the vector db."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c93e08d-8b7b-4184-9672-4f6ec0bc41c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import VectorStoreRetriever\n",
    "\n",
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "rdocs = retriever.invoke(\"what is their favorite receipe\")\n",
    "\n",
    "for rdoc in rdocs:\n",
    "    print(rdoc.page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96fc9d7-16c1-4617-baa7-0e5df0d98e12",
   "metadata": {},
   "source": [
    "Here you are seeing the results of your .invoke command."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b962b65-f56f-4533-a4fb-7c684faeec22",
   "metadata": {},
   "source": [
    "<h1>Putting it all together</h1>\n",
    "<hr>\n",
    "\n",
    "We have processed data, embeded thet and stored it in a vector database.  Now we will test it out by adding the LLM portion and performing a query to the LLM using the data from above. We will use the same model we used for the embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d044e93c-de7c-48de-a3f3-68425b477547",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "llm = OllamaLLM(\n",
    "    model=\"llama3.1:latest\",\n",
    "    base_url=\"http://192.168.15.91:11434\"\n",
    ")\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm, retriever=retriever)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc2a1d6-5918-42c3-89fe-5d206376e699",
   "metadata": {},
   "source": [
    "First question is something outside the context of the documents that have been loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4d27b0-8df2-4cda-a62b-490f0ce47925",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me about cats\"\n",
    "\n",
    "answer = qa_chain.invoke(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef80fed-ab78-4356-bfd7-0129044e1160",
   "metadata": {},
   "source": [
    "Second question is something inside the context of the documents that have been loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716a3a76-a976-4d98-91f2-04d022dce483",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me about astronauts\"\n",
    "\n",
    "answer = qa_chain.invoke(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9a3710-fd75-41b9-8656-0b26a89b45d0",
   "metadata": {},
   "source": [
    "<h1>Summary</h1>\n",
    "<hr>\n",
    "In summary we learned about each piece required for a RAG pipeline.  Keep in mind there are many other options for the technology we had reviewd in this lab, each having a different purpose and need based on the context of the data being ingested. I hope this will help you have a better understanding of the behind the scenes working of a RAG chat bot and peaks some curiosiity to look deeper into the technologies and data preping/processing methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2bb66e-8974-40f4-95bf-65a41248aa57",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
